{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深層学習スクラッチ ディープニューラルネットワーク"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【問題1】全結合層のクラス化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\" \n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.B = initializer.B(n_nodes2)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        A = np.dot(X, self.W) + self.B\n",
    "        return A\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        self.optimizer.update\n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【問題2】初期化方法のクラス化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        return self.sigma * np.random.rand(n_nodes1, n_nodes2)\n",
    "\n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        return self.sigma * np.random.rand(n_nodes2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初期化重み:\n",
      "[[0.00460154 0.00239285 0.00939916 0.0006086  0.00363186]\n",
      " [0.00968991 0.00944888 0.00760566 0.00782661 0.00104218]\n",
      " [0.00498341 0.00791552 0.00883185 0.00614388 0.00968851]\n",
      " [0.00936968 0.00144015 0.00542901 0.00816911 0.00717064]\n",
      " [0.00018168 0.00753633 0.00442851 0.00412923 0.00085417]\n",
      " [0.00212097 0.00590537 0.00978601 0.0039267  0.00826371]\n",
      " [0.00103552 0.00798286 0.00380225 0.00572066 0.00041489]\n",
      " [0.00622936 0.00963491 0.00552287 0.00342894 0.00374006]\n",
      " [0.00748851 0.00837549 0.00909923 0.002084   0.00471579]\n",
      " [0.00914756 0.00086606 0.00207528 0.00681485 0.00568636]]\n",
      "初期化バイアス:\n",
      "[0.00364833 0.00362021 0.00936003 0.00468012 0.0052603 ]\n"
     ]
    }
   ],
   "source": [
    "# 確認\n",
    "class c:\n",
    "    def __init__(self, n_nodes1, n_nodes2, initial):\n",
    "        self.W = initial.W(n_nodes1, n_nodes2)\n",
    "        self.B = initial.B(n_nodes2)\n",
    "\n",
    "a = c(n_nodes1=10, n_nodes2=5, initial=SimpleInitializer(sigma=0.01))\n",
    "print(f'初期化重み:\\n{a.W}')\n",
    "print(f'初期化バイアス:\\n{a.B}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【問題3】最適化手法のクラス化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update_W(self, layer_a, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer_z, layer_a : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        W = layer.W - self.lr * np.dot(layer.W.T, layer_a)\n",
    "        return W\n",
    "    \n",
    "    def update_B(self, layer):\n",
    "        B = layer.B - self.lr * layer.B.sum(axis=0)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【問題4】活性化関数のクラス化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class activate:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def sigmoid(self, A):\n",
    "        return 1 / (1 + np.exp(-A))\n",
    "\n",
    "    def tanh(self, A):\n",
    "        return np.tanh(A)\n",
    "\n",
    "    def softmax(self, A):\n",
    "        return np.exp(A) / np.sum(np.exp(A))\n",
    "    \n",
    "    def logsumexp(self, x):\n",
    "        \"\"\"Calculates log(sum(exp(x))).\n",
    "        \"\"\"\n",
    "        xmax = x.max(axis=1, keepdims=True)\n",
    "        return np.log(np.exp(x - xmax).sum(axis=1, keepdims=True)) + xmax\n",
    "\n",
    "    def loss(self, y, z):\n",
    "        if y.ndim == 1:\n",
    "            z = z.reshape(1, z.size)\n",
    "            y = y.reshape(1, y.size)\n",
    "        return -(y * z).mean(axis=0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【問題5】ReLUクラスの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class relu:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fwd_relu(self, A):\n",
    "        return np.where(A > 0, A, 0)\n",
    "    \n",
    "    def bck_relu(self, A):\n",
    "        return np.where(A > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\n",
      "[-5.  -4.5 -4.  -3.5 -3.  -2.5 -2.  -1.5 -1.  -0.5  0.   0.5  1.   1.5\n",
      "  2.   2.5  3.   3.5  4.   4.5]\n",
      "fwd_relu:\n",
      "[0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.5 1.  1.5 2.  2.5 3.  3.5\n",
      " 4.  4.5]\n",
      "bck_relu:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# 確認\n",
    "relu = relu()\n",
    "a = np.arange(-5, 5, 0.5)\n",
    "print(f'a:\\n{a}')\n",
    "print(f'fwd_relu:\\n{relu.fwd_relu(a)}')\n",
    "print(f'bck_relu:\\n{relu.bck_relu(a)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【問題6】重みの初期値"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class CustomInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def Xavier(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重み, バイアスの初期化:活性化関数にシグモイドやハイパボリックタンジェントの時\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        W = np.random.normal(0, np.sqrt(1 / n_nodes1), (n_nodes1, n_nodes2))\n",
    "        B = np.random.normal(0, np.sqrt(1 / n_nodes1), n_nodes2)\n",
    "        return W, B\n",
    "\n",
    "    def He(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重み, バイアスの初期化:活性化関数にReLUを使う時\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        W = np.random.normal(0, np.sqrt(2 / n_nodes1), (n_nodes1, n_nodes2))\n",
    "        B = np.random.normal(0, np.sqrt(2 / n_nodes1), n_nodes2)\n",
    "        return W, B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【問題7】最適化手法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.H_i = 0\n",
    "        self.B_i = 0\n",
    "    def update(self, layer_z, layer_a, layer):\n",
    "        self.H_i += np.dot(layer_z, layer_a) ** 2\n",
    "        self.B_i += layer_a.sum(axis=0) ** 2\n",
    "        layer.W -= self.lr * np.sqrt(1 / self.H_i) * layer_z\n",
    "        layer.B -= self.lr * np.sqrt(1 / self.B_i) * layer_a.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【問題8】クラスの完成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        return self.sigma * np.random.rand(n_nodes1, n_nodes2)\n",
    "\n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        return self.sigma * np.random.rand(n_nodes2)\n",
    "    \n",
    "class Xavier:\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        return np.random.normal(0, np.sqrt(1 / self.n_nodes1), (n_nodes1, n_nodes2))\n",
    "\n",
    "    def B(self, n_nodes2):\n",
    "        return np.random.normal(0, np.sqrt(1 / self.n_nodes1), n_nodes2)\n",
    "    \n",
    "class He:\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        return np.random.normal(0, np.sqrt(2 / self.n_nodes1), (n_nodes1, n_nodes2))\n",
    "\n",
    "    def B(self, n_nodes2):\n",
    "        return np.random.normal(0, np.sqrt(2 / self.n_nodes1), n_nodes2)\n",
    "\n",
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer_z, layer_a : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        layer.W -= self.lr * layer.dW\n",
    "        layer.B -= self.lr * layer.dB\n",
    "        return\n",
    "    \n",
    "class AdaGrad:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.H_i = 0\n",
    "        self.B_i = 0\n",
    "    def update(self, layer_z, layer_a, layer):\n",
    "        self.H_i += np.dot(layer_z, layer_a) ** 2\n",
    "        self.B_i += layer_a.sum(axis=0) ** 2\n",
    "        layer.W -= self.lr * np.sqrt(1 / self.H_i) * layer_z\n",
    "        layer.B -= self.lr * np.sqrt(1 / self.B_i) * layer_a.sum(axis=0)\n",
    "\n",
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\" \n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.B = initializer.B(n_nodes2)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        self.X = X\n",
    "        A = np.dot(X, self.W) + self.B\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        dZ = np.dot(dA, self.W.T)\n",
    "        self.dB = dA.sum(axis=0)\n",
    "        self.dW = np.dot(self.X.T, dA)\n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return dZ\n",
    "\n",
    "class actsoftmax:\n",
    "    def forward(self, x):\n",
    "        \"\"\"Calculates log(sum(exp(x))).\n",
    "        \"\"\"\n",
    "        xmax = x.max(axis=1, keepdims=True)\n",
    "        z = np.log(np.exp(x - xmax).sum(axis=1, keepdims=True)) + xmax\n",
    "        Z = x - z\n",
    "        self.Z = np.exp(Z)\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, Y, Z):\n",
    "        batch_size = Y.shape[0]\n",
    "        return (Z - Y) / batch_size\n",
    "\n",
    "    def loss_func(self, y, z):\n",
    "        if y.ndim == 1:\n",
    "            z = z.reshape(1, z.size)\n",
    "            y = y.reshape(1, y.size)\n",
    "        return -(y * z).mean(axis=0).sum()\n",
    "    \n",
    "class actsigmoid:\n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        return self.sigmoid(A)\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        bck_sig = self.sigmoid(self.A)\n",
    "        return dZ * (1 - bck_sig) * bck_sig\n",
    "    \n",
    "    def sigmoid(self, X):\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "    \n",
    "class acttanh:\n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        return np.tanh(A)\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        return dZ * (1 - (np.tanh(self.A))**2)\n",
    "    \n",
    "class actrelu:\n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        return np.clip(A, 0, None)\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        return dZ * np.clip(np.sign(self.A), 0, None)\n",
    "        \n",
    "\n",
    "class ScratchDeepNeuralNetworkClassifier:\n",
    "    \n",
    "    def __init__(self, input_size=784, hidden1_size=400, hidden2_size=200, output_size=10,\n",
    "                sigma=0.01, batch_size=20, lr=0.01, verbose=True, act=acttanh, opt=None):\n",
    "        self.verbose = verbose\n",
    "        self.input_size = input_size\n",
    "        self.hidden1_size = hidden1_size\n",
    "        self.hidden2_size = hidden2_size\n",
    "        self.output_size = output_size\n",
    "        self.sigma = sigma\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        \n",
    "        # self.sigma : ガウス分布の標準偏差\n",
    "        # self.lr : 学習率\n",
    "        # self.hidden1_size : 1層目のノード数\n",
    "        # self.hidden2_size : 2層目のノード数\n",
    "        # self.output_size : 出力層のノード数\n",
    "        # 各層の設定\n",
    "        # optimizerの設定\n",
    "        if opt == None:\n",
    "            optimizer = SGD(self.lr)\n",
    "            \n",
    "        elif opt == 'Ada':\n",
    "            optimizer = AdaGrad(self.lr)\n",
    "            \n",
    "        if act == acttanh:\n",
    "            initial = SimpleInitializer(self.sigma)\n",
    "            \n",
    "        elif act == actsigmoid:\n",
    "            initial = Xavier(self.sigma)\n",
    "            \n",
    "        elif act == actrelu:\n",
    "            initial = He(self.sigma)\n",
    "        \n",
    "        self.FC1 = FC(self.input_size, self.hidden1_size, initial, optimizer)\n",
    "        self.activation1 = act() # tanh\n",
    "        self.FC2 = FC(self.hidden1_size, self.hidden2_size, initial, optimizer)\n",
    "        self.activation2 = act() # tanh\n",
    "        self.FC3 = FC(self.hidden2_size, self.output_size, initial, optimizer)\n",
    "        self.activation3 = actsoftmax() # Softmax\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # イテレーション毎の順伝搬\n",
    "        A1 = self.FC1.forward(X)\n",
    "        Z1 = self.activation1.forward(A1)\n",
    "        A2 = self.FC2.forward(Z1)\n",
    "        Z2 = self.activation2.forward(A2)\n",
    "        A3 = self.FC3.forward(Z2)\n",
    "        Z3 = self.activation3.forward(A3)\n",
    "        return Z3\n",
    "    \n",
    "    def backward(self, y):\n",
    "        # イテレーション毎の逆伝搬\n",
    "        dA3 = self.activation3.backward(y, self.activation3.Z) # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "        dZ2 = self.FC3.backward(dA3)\n",
    "        dA2 = self.activation2.backward(dZ2)\n",
    "        dZ1 = self.FC2.backward(dA2)\n",
    "        dA1 = self.activation1.backward(dZ1)\n",
    "        dZ0 = self.FC1.backward(dA1) # dZ0は使用しない\n",
    "    \n",
    "    # 正解率を出力\n",
    "    def accuracy(self, y, z):\n",
    "        return (z.argmax(axis=1) == y).sum()\n",
    "        \n",
    "    def fit(self, X, y, X_val=None, y_val=None, epochs=20):\n",
    "        div_iter = 600\n",
    "        plot_data = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size)\n",
    "            loss_sum = 0\n",
    "            for i in range(len(get_mini_batch)):\n",
    "                X_train, y_train = get_mini_batch[i]\n",
    "                batch_size = X_train.shape[0]\n",
    "                self.X = X_train.reshape(batch_size, 784)\n",
    "                self.y = (y_train.reshape(-1, 1) == np.arange(10)).astype(np.float64)\n",
    "\n",
    "#                 print(self.activation3.Z)\n",
    "                Z3 = self.forward(self.X)\n",
    "                loss = self.activation3.loss_func(self.y, Z3)\n",
    "                self.backward(self.y)\n",
    "                loss_sum += loss\n",
    "                \n",
    "                if self.verbose and (i + 1) % div_iter == 0:\n",
    "                    train_loss = loss_sum / div_iter\n",
    "                    self.y_val = (y_val.reshape(-1, 1) == np.arange(10)).astype(np.float64)\n",
    "                    self.X_val = X_val.reshape(X_val.shape[0], 784)\n",
    "                    val_Z3 = self.forward(self.X_val)\n",
    "                    val_loss = self.activation3.loss_func(self.y_val, val_Z3)\n",
    "                    val_accuracy = self.accuracy(y_val, self.activation3.Z)\n",
    "                    \n",
    "                    print(f'epoch: {epoch + 1} / {epochs}, iteration: {i + 1} / {len(get_mini_batch)} | train loss : {train_loss:.3} | val loss: {val_loss:.3} | accuracy: {val_accuracy / len(y_val)}')\n",
    "                    loss_sum = 0\n",
    "                    \n",
    "                    iters_per_epoch = len(X) / self.batch_size\n",
    "                    plot_data.append((epoch + (i + 1) / iters_per_epoch, train_loss, val_loss))\n",
    "                    \n",
    "        if self.verbose:\n",
    "            epochs, train_loss, val_loss = zip(*plot_data)\n",
    "            plt.plot(epochs, train_loss, color='g', label='train loss')\n",
    "            plt.plot(epochs, val_loss, color='b', label='val loss')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.ylabel('loss')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "                \n",
    "    def predict(self, X):\n",
    "        A1 = self.FC1.forward(X)\n",
    "        Z1 = self.activation1.forward(A1)\n",
    "        A2 = self.FC2.forward(Z1)\n",
    "        Z2 = self.activation2.forward(A2)\n",
    "        A3 = self.FC3.forward(Z2)\n",
    "        Z3 = self.activation3.forward(A3)\n",
    "        \n",
    "        return np.argmax(self.activation3.Z, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【問題9】学習と推定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ミニバッチ処理のサンプルクラス\n",
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (10000, 784)\n",
      "0.0 1.0\n",
      "0.0 1.0\n",
      "(48000, 784) (12000, 784)\n",
      "(48000,) (12000,)\n"
     ]
    }
   ],
   "source": [
    "# MNISTダウンロード\n",
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 用意した画像データセットを(サンプル数, 一次元の画素数)型に変換\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "# 画素値を正規化処理\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.min(), X_train.max())\n",
    "print(X_test.min(), X_test.max())\n",
    "\n",
    "# trainとvalデータに分割\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "print(X_train.shape, X_val.shape)\n",
    "print(y_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_mini_batch = GetMiniBatch(X_train, y_train, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        return self.sigma * np.random.rand(n_nodes1, n_nodes2)\n",
    "\n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        return self.sigma * np.random.rand(n_nodes2)\n",
    "    \n",
    "class Xavier:\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        return np.random.normal(0, np.sqrt(1 / self.n_nodes1), (n_nodes1, n_nodes2))\n",
    "\n",
    "    def B(self, n_nodes2):\n",
    "        return np.random.normal(0, np.sqrt(1 / self.n_nodes1), n_nodes2)\n",
    "    \n",
    "class He:\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        return np.random.normal(0, np.sqrt(2 / self.n_nodes1), (n_nodes1, n_nodes2))\n",
    "\n",
    "    def B(self, n_nodes2):\n",
    "        return np.random.normal(0, np.sqrt(2 / self.n_nodes1), n_nodes2)\n",
    "\n",
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer_z, layer_a : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        layer.W -= self.lr * layer.dW\n",
    "        layer.B -= self.lr * layer.dB\n",
    "        return\n",
    "    \n",
    "class AdaGrad:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.H_i = 0\n",
    "        self.B_i = 0\n",
    "    def update(self, layer_z, layer_a, layer):\n",
    "        self.H_i += np.dot(layer_z, layer_a) ** 2\n",
    "        self.B_i += layer_a.sum(axis=0) ** 2\n",
    "        layer.W -= self.lr * np.sqrt(1 / self.H_i) * layer_z\n",
    "        layer.B -= self.lr * np.sqrt(1 / self.B_i) * layer_a.sum(axis=0)\n",
    "\n",
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\" \n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.B = initializer.B(n_nodes2)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        self.X = X\n",
    "        A = np.dot(X, self.W) + self.B\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        dZ = np.dot(dA, self.W.T)\n",
    "        self.dB = dA.sum(axis=0)\n",
    "        self.dW = np.dot(self.X.T, dA)\n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return dZ\n",
    "\n",
    "class actsoftmax:\n",
    "    def forward(self, x):\n",
    "        \"\"\"Calculates log(sum(exp(x))).\n",
    "        \"\"\"\n",
    "        xmax = x.max(axis=1, keepdims=True)\n",
    "        z = np.log(np.exp(x - xmax).sum(axis=1, keepdims=True)) + xmax\n",
    "        Z = x - z\n",
    "        self.Z = np.exp(Z)\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, Y, Z):\n",
    "        batch_size = Y.shape[0]\n",
    "        return (Z - Y) / batch_size\n",
    "\n",
    "    def loss_func(self, y, z):\n",
    "        if y.ndim == 1:\n",
    "            z = z.reshape(1, z.size)\n",
    "            y = y.reshape(1, y.size)\n",
    "        return -(y * z).mean(axis=0).sum()\n",
    "    \n",
    "class actsigmoid:\n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        return self.sigmoid(A)\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        bck_sig = self.sigmoid(self.A)\n",
    "        return dZ * (1 - bck_sig) * bck_sig\n",
    "    \n",
    "    def sigmoid(self, X):\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "    \n",
    "class acttanh:\n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        return np.tanh(A)\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        return dZ * (1 - (np.tanh(self.A))**2)\n",
    "    \n",
    "class actrelu:\n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        return np.clip(A, 0, None)\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        return dZ * np.clip(np.sign(self.A), 0, None)\n",
    "        \n",
    "\n",
    "class ScratchDeepNeuralNetworkClassifier:\n",
    "    \n",
    "    def __init__(self, input_size=784, hidden1_size=400, hidden2_size=200, output_size=10,\n",
    "                sigma=0.01, batch_size=20, lr=0.01, verbose=True, act=acttanh, opt=None):\n",
    "        self.verbose = verbose\n",
    "        self.input_size = input_size\n",
    "        self.hidden1_size = hidden1_size\n",
    "        self.hidden2_size = hidden2_size\n",
    "        self.output_size = output_size\n",
    "        self.sigma = sigma\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        \n",
    "        # self.sigma : ガウス分布の標準偏差\n",
    "        # self.lr : 学習率\n",
    "        # self.hidden1_size : 1層目のノード数\n",
    "        # self.hidden2_size : 2層目のノード数\n",
    "        # self.output_size : 出力層のノード数\n",
    "        # 各層の設定\n",
    "        # optimizerの設定\n",
    "        if opt == None:\n",
    "            optimizer = SGD(self.lr)\n",
    "            \n",
    "        elif opt == 'Ada':\n",
    "            optimizer = AdaGrad(self.lr)\n",
    "            \n",
    "        # activateの設定\n",
    "        if act == acttanh:\n",
    "            initial = SimpleInitializer(self.sigma)\n",
    "            \n",
    "        elif act == actsigmoid:\n",
    "            initial = Xavier(self.sigma)\n",
    "            \n",
    "        elif act == actrelu:\n",
    "            initial = He(self.sigma)\n",
    "        \n",
    "        self.FC1 = FC(self.input_size, self.hidden1_size, initial, optimizer)\n",
    "        self.activation1 = act() # tanh\n",
    "        self.FC2 = FC(self.hidden1_size, self.hidden2_size, initial, optimizer)\n",
    "        self.activation2 = act() # tanh\n",
    "        self.FC3 = FC(self.hidden2_size, self.output_size, initial, optimizer)\n",
    "        self.activation3 = actsoftmax() # Softmax\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # イテレーション毎の順伝搬\n",
    "        A1 = self.FC1.forward(X)\n",
    "        Z1 = self.activation1.forward(A1)\n",
    "        A2 = self.FC2.forward(Z1)\n",
    "        Z2 = self.activation2.forward(A2)\n",
    "        A3 = self.FC3.forward(Z2)\n",
    "        Z3 = self.activation3.forward(A3)\n",
    "        return Z3\n",
    "    \n",
    "    def backward(self, y):\n",
    "        # イテレーション毎の逆伝搬\n",
    "        dA3 = self.activation3.backward(y, self.activation3.Z) # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "        dZ2 = self.FC3.backward(dA3)\n",
    "        dA2 = self.activation2.backward(dZ2)\n",
    "        dZ1 = self.FC2.backward(dA2)\n",
    "        dA1 = self.activation1.backward(dZ1)\n",
    "        dZ0 = self.FC1.backward(dA1) # dZ0は使用しない\n",
    "    \n",
    "    # 正解率を出力\n",
    "    def accuracy(self, y, z):\n",
    "        return (z.argmax(axis=1) == y).sum()\n",
    "        \n",
    "    def fit(self, X, y, X_val=None, y_val=None, epochs=20):\n",
    "        div_iter = 600\n",
    "        plot_data = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size)\n",
    "            loss_sum = 0\n",
    "            for i in range(len(get_mini_batch)):\n",
    "                X_train, y_train = get_mini_batch[i]\n",
    "                batch_size = X_train.shape[0]\n",
    "                self.X = X_train.reshape(batch_size, 704)\n",
    "                self.y = (y_train.reshape(-1, 1) == np.arange(10)).astype(np.float64)\n",
    "\n",
    "#                 print(self.activation3.Z)\n",
    "                Z3 = self.forward(self.X)\n",
    "                loss = self.activation3.loss_func(self.y, Z3)\n",
    "                self.backward(self.y)\n",
    "                loss_sum += loss\n",
    "                \n",
    "                if self.verbose and (i + 1) % div_iter == 0:\n",
    "                    train_loss = loss_sum / div_iter\n",
    "                    self.y_val = (y_val.reshape(-1, 1) == np.arange(10)).astype(np.float64)\n",
    "                    self.X_val = X_val.reshape(X_val.shape[0], 784)\n",
    "                    val_Z3 = self.forward(self.X_val)\n",
    "                    val_loss = self.activation3.loss_func(self.y_val, val_Z3)\n",
    "                    val_accuracy = self.accuracy(y_val, self.activation3.Z)\n",
    "                    \n",
    "                    print(f'epoch: {epoch + 1} / {epochs}, iteration: {i + 1} / {len(get_mini_batch)} | train loss : {train_loss:.3} | val loss: {val_loss:.3} | accuracy: {val_accuracy / len(y_val)}')\n",
    "                    loss_sum = 0\n",
    "                    \n",
    "                    iters_per_epoch = len(X) / self.batch_size\n",
    "                    plot_data.append((epoch + (i + 1) / iters_per_epoch, train_loss, val_loss))\n",
    "                    \n",
    "        if self.verbose:\n",
    "            epochs, train_loss, val_loss = zip(*plot_data)\n",
    "            plt.plot(epochs, train_loss, color='g', label='train loss')\n",
    "            plt.plot(epochs, val_loss, color='b', label='val loss')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.ylabel('loss')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "                \n",
    "    def predict(self, X):\n",
    "        A1 = self.FC1.forward(X)\n",
    "        Z1 = self.activation1.forward(A1)\n",
    "        A2 = self.FC2.forward(Z1)\n",
    "        Z2 = self.activation2.forward(A2)\n",
    "        A3 = self.FC3.forward(Z2)\n",
    "        Z3 = self.activation3.forward(A3)\n",
    "        \n",
    "        return np.argmax(self.activation3.Z, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 15680 into shape (20,704)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d7fe13a736f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 3層, 初期値ガウス分布、tanh使用の結果\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mScratchDeepNeuralNetworkClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-07d0d88c734f>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, X_val, y_val, epochs)\u001b[0m\n\u001b[1;32m    277\u001b[0m                 \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_mini_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                 \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m704\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 15680 into shape (20,704)"
     ]
    }
   ],
   "source": [
    "# 3層, 初期値ガウス分布、tanh使用の結果\n",
    "network = ScratchDeepNeuralNetworkClassifier()\n",
    "network.fit(X_train, y_train, X_val, y_val, epochs=10)\n",
    "pred = network.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "print(f'予測結果の正解率: {metrics.accuracy_score(y_test, pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 / 10, iteration: 600 / 2400 | train loss : 2.29 | val loss: 2.26 | accuracy: 0.11341666666666667\n",
      "epoch: 1 / 10, iteration: 1200 / 2400 | train loss : 2.24 | val loss: 2.21 | accuracy: 0.16125\n",
      "epoch: 1 / 10, iteration: 1800 / 2400 | train loss : 2.17 | val loss: 2.12 | accuracy: 0.5196666666666667\n",
      "epoch: 1 / 10, iteration: 2400 / 2400 | train loss : 2.05 | val loss: 1.96 | accuracy: 0.4374166666666667\n",
      "epoch: 2 / 10, iteration: 600 / 2400 | train loss : 1.85 | val loss: 1.72 | accuracy: 0.6255833333333334\n",
      "epoch: 2 / 10, iteration: 1200 / 2400 | train loss : 1.59 | val loss: 1.45 | accuracy: 0.6208333333333333\n",
      "epoch: 2 / 10, iteration: 1800 / 2400 | train loss : 1.34 | val loss: 1.22 | accuracy: 0.72425\n",
      "epoch: 2 / 10, iteration: 2400 / 2400 | train loss : 1.13 | val loss: 1.05 | accuracy: 0.7104166666666667\n",
      "epoch: 3 / 10, iteration: 600 / 2400 | train loss : 0.978 | val loss: 0.917 | accuracy: 0.764\n",
      "epoch: 3 / 10, iteration: 1200 / 2400 | train loss : 0.868 | val loss: 0.822 | accuracy: 0.7755\n",
      "epoch: 3 / 10, iteration: 1800 / 2400 | train loss : 0.781 | val loss: 0.746 | accuracy: 0.8054166666666667\n",
      "epoch: 3 / 10, iteration: 2400 / 2400 | train loss : 0.717 | val loss: 0.688 | accuracy: 0.8135833333333333\n",
      "epoch: 4 / 10, iteration: 600 / 2400 | train loss : 0.654 | val loss: 0.639 | accuracy: 0.8263333333333334\n",
      "epoch: 4 / 10, iteration: 1200 / 2400 | train loss : 0.618 | val loss: 0.603 | accuracy: 0.8325833333333333\n",
      "epoch: 4 / 10, iteration: 1800 / 2400 | train loss : 0.582 | val loss: 0.568 | accuracy: 0.8465833333333334\n",
      "epoch: 4 / 10, iteration: 2400 / 2400 | train loss : 0.558 | val loss: 0.543 | accuracy: 0.8494166666666667\n",
      "epoch: 5 / 10, iteration: 600 / 2400 | train loss : 0.519 | val loss: 0.52 | accuracy: 0.8560833333333333\n",
      "epoch: 5 / 10, iteration: 1200 / 2400 | train loss : 0.508 | val loss: 0.503 | accuracy: 0.86175\n",
      "epoch: 5 / 10, iteration: 1800 / 2400 | train loss : 0.488 | val loss: 0.483 | accuracy: 0.8685833333333334\n",
      "epoch: 5 / 10, iteration: 2400 / 2400 | train loss : 0.479 | val loss: 0.469 | accuracy: 0.8670833333333333\n",
      "epoch: 6 / 10, iteration: 600 / 2400 | train loss : 0.451 | val loss: 0.458 | accuracy: 0.87275\n",
      "epoch: 6 / 10, iteration: 1200 / 2400 | train loss : 0.447 | val loss: 0.447 | accuracy: 0.8754166666666666\n",
      "epoch: 6 / 10, iteration: 1800 / 2400 | train loss : 0.435 | val loss: 0.434 | accuracy: 0.8804166666666666\n",
      "epoch: 6 / 10, iteration: 2400 / 2400 | train loss : 0.434 | val loss: 0.426 | accuracy: 0.8778333333333334\n",
      "epoch: 7 / 10, iteration: 600 / 2400 | train loss : 0.41 | val loss: 0.421 | accuracy: 0.8823333333333333\n",
      "epoch: 7 / 10, iteration: 1200 / 2400 | train loss : 0.41 | val loss: 0.413 | accuracy: 0.8846666666666667\n",
      "epoch: 7 / 10, iteration: 1800 / 2400 | train loss : 0.401 | val loss: 0.404 | accuracy: 0.88875\n",
      "epoch: 7 / 10, iteration: 2400 / 2400 | train loss : 0.405 | val loss: 0.398 | accuracy: 0.8851666666666667\n",
      "epoch: 8 / 10, iteration: 600 / 2400 | train loss : 0.384 | val loss: 0.396 | accuracy: 0.8888333333333334\n",
      "epoch: 8 / 10, iteration: 1200 / 2400 | train loss : 0.386 | val loss: 0.39 | accuracy: 0.8908333333333334\n",
      "epoch: 8 / 10, iteration: 1800 / 2400 | train loss : 0.379 | val loss: 0.384 | accuracy: 0.8920833333333333\n",
      "epoch: 8 / 10, iteration: 2400 / 2400 | train loss : 0.384 | val loss: 0.379 | accuracy: 0.8900833333333333\n",
      "epoch: 9 / 10, iteration: 600 / 2400 | train loss : 0.366 | val loss: 0.379 | accuracy: 0.8941666666666667\n",
      "epoch: 9 / 10, iteration: 1200 / 2400 | train loss : 0.368 | val loss: 0.374 | accuracy: 0.8944166666666666\n",
      "epoch: 9 / 10, iteration: 1800 / 2400 | train loss : 0.362 | val loss: 0.369 | accuracy: 0.8968333333333334\n",
      "epoch: 9 / 10, iteration: 2400 / 2400 | train loss : 0.369 | val loss: 0.365 | accuracy: 0.8946666666666667\n",
      "epoch: 10 / 10, iteration: 600 / 2400 | train loss : 0.353 | val loss: 0.365 | accuracy: 0.8963333333333333\n",
      "epoch: 10 / 10, iteration: 1200 / 2400 | train loss : 0.354 | val loss: 0.361 | accuracy: 0.8969166666666667\n",
      "epoch: 10 / 10, iteration: 1800 / 2400 | train loss : 0.349 | val loss: 0.357 | accuracy: 0.899\n",
      "epoch: 10 / 10, iteration: 2400 / 2400 | train loss : 0.357 | val loss: 0.354 | accuracy: 0.8975833333333333\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "予測結果の正解率: 0.9037\n"
     ]
    }
   ],
   "source": [
    "# 3層, 初期値Xahiber, 活性化sigmoid使用の結果\n",
    "network = ScratchDeepNeuralNetworkClassifier(act=actsigmoid)\n",
    "network.fit(X_train, y_train, X_val, y_val, epochs=10)\n",
    "pred = network.predict(X_test)\n",
    "\n",
    "print(f'予測結果の正解率: {metrics.accuracy_score(y_test, pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 / 10, iteration: 600 / 2400 | train loss : 0.876 | val loss: 0.459 | accuracy: 0.8749166666666667\n",
      "epoch: 1 / 10, iteration: 1200 / 2400 | train loss : 0.396 | val loss: 0.355 | accuracy: 0.8979166666666667\n",
      "epoch: 1 / 10, iteration: 1800 / 2400 | train loss : 0.329 | val loss: 0.311 | accuracy: 0.9140833333333334\n",
      "epoch: 1 / 10, iteration: 2400 / 2400 | train loss : 0.304 | val loss: 0.279 | accuracy: 0.9185\n",
      "epoch: 2 / 10, iteration: 600 / 2400 | train loss : 0.268 | val loss: 0.27 | accuracy: 0.9215833333333333\n",
      "epoch: 2 / 10, iteration: 1200 / 2400 | train loss : 0.247 | val loss: 0.248 | accuracy: 0.9266666666666666\n",
      "epoch: 2 / 10, iteration: 1800 / 2400 | train loss : 0.231 | val loss: 0.232 | accuracy: 0.9333333333333333\n",
      "epoch: 2 / 10, iteration: 2400 / 2400 | train loss : 0.23 | val loss: 0.216 | accuracy: 0.9355833333333333\n",
      "epoch: 3 / 10, iteration: 600 / 2400 | train loss : 0.207 | val loss: 0.215 | accuracy: 0.9365\n",
      "epoch: 3 / 10, iteration: 1200 / 2400 | train loss : 0.193 | val loss: 0.203 | accuracy: 0.9409166666666666\n",
      "epoch: 3 / 10, iteration: 1800 / 2400 | train loss : 0.183 | val loss: 0.191 | accuracy: 0.9440833333333334\n",
      "epoch: 3 / 10, iteration: 2400 / 2400 | train loss : 0.187 | val loss: 0.179 | accuracy: 0.9463333333333334\n",
      "epoch: 4 / 10, iteration: 600 / 2400 | train loss : 0.169 | val loss: 0.18 | accuracy: 0.94625\n",
      "epoch: 4 / 10, iteration: 1200 / 2400 | train loss : 0.158 | val loss: 0.173 | accuracy: 0.9489166666666666\n",
      "epoch: 4 / 10, iteration: 1800 / 2400 | train loss : 0.151 | val loss: 0.164 | accuracy: 0.9510833333333333\n",
      "epoch: 4 / 10, iteration: 2400 / 2400 | train loss : 0.157 | val loss: 0.155 | accuracy: 0.9525833333333333\n",
      "epoch: 5 / 10, iteration: 600 / 2400 | train loss : 0.141 | val loss: 0.156 | accuracy: 0.952\n",
      "epoch: 5 / 10, iteration: 1200 / 2400 | train loss : 0.134 | val loss: 0.153 | accuracy: 0.9544166666666667\n",
      "epoch: 5 / 10, iteration: 1800 / 2400 | train loss : 0.128 | val loss: 0.145 | accuracy: 0.956\n",
      "epoch: 5 / 10, iteration: 2400 / 2400 | train loss : 0.135 | val loss: 0.137 | accuracy: 0.95825\n",
      "epoch: 6 / 10, iteration: 600 / 2400 | train loss : 0.121 | val loss: 0.139 | accuracy: 0.9576666666666667\n",
      "epoch: 6 / 10, iteration: 1200 / 2400 | train loss : 0.116 | val loss: 0.138 | accuracy: 0.95825\n",
      "epoch: 6 / 10, iteration: 1800 / 2400 | train loss : 0.111 | val loss: 0.131 | accuracy: 0.9598333333333333\n",
      "epoch: 6 / 10, iteration: 2400 / 2400 | train loss : 0.117 | val loss: 0.125 | accuracy: 0.9629166666666666\n",
      "epoch: 7 / 10, iteration: 600 / 2400 | train loss : 0.105 | val loss: 0.126 | accuracy: 0.9615\n",
      "epoch: 7 / 10, iteration: 1200 / 2400 | train loss : 0.101 | val loss: 0.127 | accuracy: 0.9615833333333333\n",
      "epoch: 7 / 10, iteration: 1800 / 2400 | train loss : 0.0973 | val loss: 0.121 | accuracy: 0.9634166666666667\n",
      "epoch: 7 / 10, iteration: 2400 / 2400 | train loss : 0.103 | val loss: 0.115 | accuracy: 0.9655833333333333\n",
      "epoch: 8 / 10, iteration: 600 / 2400 | train loss : 0.0925 | val loss: 0.116 | accuracy: 0.964\n",
      "epoch: 8 / 10, iteration: 1200 / 2400 | train loss : 0.0893 | val loss: 0.119 | accuracy: 0.9644166666666667\n",
      "epoch: 8 / 10, iteration: 1800 / 2400 | train loss : 0.0864 | val loss: 0.113 | accuracy: 0.96575\n",
      "epoch: 8 / 10, iteration: 2400 / 2400 | train loss : 0.0915 | val loss: 0.108 | accuracy: 0.9679166666666666\n",
      "epoch: 9 / 10, iteration: 600 / 2400 | train loss : 0.0821 | val loss: 0.108 | accuracy: 0.96725\n",
      "epoch: 9 / 10, iteration: 1200 / 2400 | train loss : 0.0796 | val loss: 0.112 | accuracy: 0.9659166666666666\n",
      "epoch: 9 / 10, iteration: 1800 / 2400 | train loss : 0.0772 | val loss: 0.107 | accuracy: 0.9683333333333334\n",
      "epoch: 9 / 10, iteration: 2400 / 2400 | train loss : 0.0814 | val loss: 0.102 | accuracy: 0.969\n",
      "epoch: 10 / 10, iteration: 600 / 2400 | train loss : 0.0735 | val loss: 0.102 | accuracy: 0.9688333333333333\n",
      "epoch: 10 / 10, iteration: 1200 / 2400 | train loss : 0.0713 | val loss: 0.106 | accuracy: 0.9671666666666666\n",
      "epoch: 10 / 10, iteration: 1800 / 2400 | train loss : 0.0692 | val loss: 0.102 | accuracy: 0.9690833333333333\n",
      "epoch: 10 / 10, iteration: 2400 / 2400 | train loss : 0.0729 | val loss: 0.0981 | accuracy: 0.97025\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "予測結果の正解率: 0.9716\n"
     ]
    }
   ],
   "source": [
    "# 3層, 初期値He、ReLU使用の結果\n",
    "network = ScratchDeepNeuralNetworkClassifier(act=actrelu)\n",
    "network.fit(X_train, y_train, X_val, y_val, epochs=10)\n",
    "pred = network.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "print(f'予測結果の正解率: {metrics.accuracy_score(y_test, pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
