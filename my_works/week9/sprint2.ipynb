{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深層学習スクラッチ 畳み込みニューラルネットワーク1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【問題1】チャンネル数を1に限定した1次元畳み込み層クラスの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimpleConv1d:\n",
    "    def __init__(self, N_in=np.array([1,2,3,4]), padding=0, filter_size=np.array([3, 5, 7]), stride=1):\n",
    "        self.w = filter_size\n",
    "        self.b = np.array([1])\n",
    "        self.x = N_in\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.N_out = self.compute_out(N_in=self.x, padding=self.padding, filter_size=self.w, stride=self.stride)\n",
    "        \n",
    "    def compute_out(self, N_in, padding, filter_size, stride):\n",
    "        N_out_shape = (len(N_in) + 2 * padding - len(filter_size)) / stride + 1\n",
    "        return np.zeros(int(N_out_shape))\n",
    "        \n",
    "    def forward(self):\n",
    "        for i in range(len(self.N_out)):\n",
    "            self.N_out[i] = np.sum(self.x[0+i:len(self.w)+i] * self.w, axis=0) + 1\n",
    "            \n",
    "    def backward(self, loss=np.array([10, 20])):\n",
    "        self.delta_w = np.zeros(self.w.size)\n",
    "        self.delta_x = np.zeros(self.x.size)\n",
    "        self.delta_b = np.sum(loss)\n",
    "        for i in range(self.w.size):\n",
    "            self.delta_w[i] = np.sum(loss * self.x[0+i:len(loss)+i])\n",
    "            \n",
    "        for j in range(self.x.size):\n",
    "            for s in range(self.w.size):\n",
    "                loss_index = j - s\n",
    "                if loss_index < 0 or loss_index > self.N_out.size - 1:\n",
    "                    pass\n",
    "                else:\n",
    "                    self.delta_x[j] += loss[loss_index] * self.w[s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【問題2】1次元畳み込み後の出力サイズの計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_out(N_in, padding, filter_size, stride):\n",
    "    return (N_in + 2 * padding - filter_size) / stride + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "計算された出力サイズ：8.0\n"
     ]
    }
   ],
   "source": [
    "# 確認\n",
    "N_in = 10\n",
    "padding = 0\n",
    "filter_size = 3\n",
    "stride = 1\n",
    "print(f'計算された出力サイズ：{compute_out(N_in, padding, filter_size, stride)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【問題3】小さな配列での1次元畳み込み層の実験"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35. 50.]\n",
      "[ 50.  80. 110.] 30 [ 30. 110. 170. 140.]\n"
     ]
    }
   ],
   "source": [
    "simple_comv = SimpleConv1d()\n",
    "\n",
    "# 順伝搬確認\n",
    "simple_comv.forward()\n",
    "print(simple_comv.N_out)\n",
    "\n",
    "# 逆伝搬確認\n",
    "simple_comv.backward()\n",
    "print(simple_comv.delta_w, simple_comv.delta_b, simple_comv.delta_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【問題4】チャンネル数を限定しない1次元畳み込み層クラスの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]]) # shape(2, 4)で、（入力チャンネル数、特徴量数）である。\n",
    "w = np.ones((3, 2, 3)) # 例の簡略化のため全て1とする。(出力チャンネル数、入力チャンネル数、フィルタサイズ)である。\n",
    "b = np.array([1, 2, 3]) # （出力チャンネル数）\n",
    "\n",
    "a = np.array([[16, 22], [17, 23], [18, 24]]) # shape(3, 2)で、（出力チャンネル数、特徴量数）である。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MultiConv1d:\n",
    "    def __init__(self, N_in=None, filter_size=None, stride=1, padding=0):\n",
    "        self.w = filter_size\n",
    "        self.b = np.array([1, 2, 3]) # 題意に沿って設定\n",
    "        self.x = N_in\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.N_out = np.zeros((self.w.shape[0], self.w.shape[1]))\n",
    "        \n",
    "    def forward(self):\n",
    "        for i in range(self.N_out.shape[0]):\n",
    "            for j in range(self.N_out.shape[1]):\n",
    "                self.N_out[i, j] = np.sum(self.w[i].reshape(-1) * self.x[:, j:self.w[i].shape[1]+j].reshape(-1)) + self.b[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16. 22.]\n",
      " [17. 23.]\n",
      " [18. 24.]]\n"
     ]
    }
   ],
   "source": [
    "multi = MultiConv1d(N_in=x, filter_size=w)\n",
    "\n",
    "# 順伝搬確認\n",
    "multi.forward()\n",
    "print(multi.N_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【問題5】（アドバンス課題）パディングの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【問題6】（アドバンス課題）ミニバッチへの対応"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【問題7】（アドバンス課題）任意のストライド数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【問題8】学習と推定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        return self.sigma * np.random.rand(n_nodes1, n_nodes2)\n",
    "\n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        return self.sigma * np.random.rand(n_nodes2)\n",
    "        \n",
    "    \n",
    "class SimpleConv1d:\n",
    "    # ここに入るn_nodes1:バッチサイズ×画素数（20 * 784）\n",
    "    def __init__(self, stride=1, padding=0):\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        self.filter_size = 7681\n",
    "        self.W = np.random.rand(self.filter_size)\n",
    "        self.B = np.array([1])\n",
    "        self.lr = 0.01\n",
    "        \n",
    "    def compute_out(self, N_in, padding, filter_size, stride):\n",
    "        N_out_shape = (len(N_in) + 2 * padding - filter_size) / stride + 1\n",
    "        return np.zeros(int(N_out_shape))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        self.x = X\n",
    "        N_out = self.compute_out(X, self.padding, self.filter_size, self.stride)\n",
    "        for i in range(len(N_out)):\n",
    "            N_out[i] = np.sum(self.x[0+i:self.W.shape[0]+i] * self.W, axis=0) + 1\n",
    "        return N_out\n",
    "            \n",
    "    def backward(self, dA):\n",
    "        self.delta_w = np.zeros(self.W.size)\n",
    "        self.delta_x = np.zeros(self.x.size)\n",
    "        self.delta_b = np.sum(dA)\n",
    "        self.B = self.B - self.lr * self.delta_b\n",
    "        for i in range(self.W.size):\n",
    "            self.delta_w[i] = np.sum(dA * self.x[0+i:len(dA)+i])\n",
    "            self.W[i] -= self.lr * self.delta_w[i]\n",
    "            \n",
    "#         for j in range(self.x.size):\n",
    "#             for s in range(self.W.size):\n",
    "#                 loss_index = j - s\n",
    "#                 if loss_index < 0 or loss_index > dA.size - 1:\n",
    "#                     pass\n",
    "#                 else:\n",
    "#                     self.delta_x[j] += dA[loss_index] * self.W[s]\n",
    "                    \n",
    "        dZ = self.delta_x\n",
    "        return dZ\n",
    "    \n",
    "class Xavier:\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        return np.random.normal(0, np.sqrt(1 / self.n_nodes1), (n_nodes1, n_nodes2))\n",
    "\n",
    "    def B(self, n_nodes2):\n",
    "        return np.random.normal(0, np.sqrt(1 / self.n_nodes1), n_nodes2)\n",
    "    \n",
    "class He:\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        return np.random.normal(0, np.sqrt(2 / self.n_nodes1), (n_nodes1, n_nodes2))\n",
    "\n",
    "    def B(self, n_nodes2):\n",
    "        return np.random.normal(0, np.sqrt(2 / self.n_nodes1), n_nodes2)\n",
    "\n",
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer_z, layer_a : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        layer.W -= self.lr * layer.dW\n",
    "        layer.B -= self.lr * layer.dB\n",
    "        return\n",
    "    \n",
    "class AdaGrad:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.H_i = 0\n",
    "        self.B_i = 0\n",
    "    def update(self, layer_z, layer_a, layer):\n",
    "        self.H_i += np.dot(layer_z, layer_a) ** 2\n",
    "        self.B_i += layer_a.sum(axis=0) ** 2\n",
    "        layer.W -= self.lr * np.sqrt(1 / self.H_i) * layer_z\n",
    "        layer.B -= self.lr * np.sqrt(1 / self.B_i) * layer_a.sum(axis=0)\n",
    "\n",
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\" \n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.B = initializer.B(n_nodes2)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        self.X = X\n",
    "        A = np.dot(X, self.W) + self.B\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        dZ = np.dot(dA, self.W.T)\n",
    "        self.dB = dA.sum(axis=0)\n",
    "        self.dW = np.dot(self.X.T, dA)\n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return dZ\n",
    "\n",
    "class actsoftmax:\n",
    "    def forward(self, x):\n",
    "        \"\"\"Calculates log(sum(exp(x))).\n",
    "        \"\"\"\n",
    "        xmax = x.max(axis=1, keepdims=True)\n",
    "        z = np.log(np.exp(x - xmax).sum(axis=1, keepdims=True)) + xmax\n",
    "        Z = x - z\n",
    "        self.Z = np.exp(Z)\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, Y, Z):\n",
    "        batch_size = Y.shape[0]\n",
    "        return (Z - Y) / batch_size\n",
    "\n",
    "    def loss_func(self, y, z):\n",
    "        if y.ndim == 1:\n",
    "            z = z.reshape(1, z.size)\n",
    "            y = y.reshape(1, y.size)\n",
    "        return -(y * z).mean(axis=0).sum()\n",
    "    \n",
    "class actsigmoid:\n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        return self.sigmoid(A)\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        bck_sig = self.sigmoid(self.A)\n",
    "        return dZ * (1 - bck_sig) * bck_sig\n",
    "    \n",
    "    def sigmoid(self, X):\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "    \n",
    "class acttanh:\n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        return np.tanh(A)\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        return dZ * (1 - (np.tanh(self.A))**2)\n",
    "    \n",
    "class actrelu:\n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        return np.clip(A, 0, None)\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        return dZ * np.clip(np.sign(self.A), 0, None)\n",
    "        \n",
    "\n",
    "class ScratchDeepNeuralNetworkClassifier:\n",
    "    \n",
    "    def __init__(self, input_size=784, hidden1_size=400, hidden2_size=200, output_size=10,\n",
    "                sigma=0.01, batch_size=20, lr=0.01, verbose=True, act=acttanh, opt=None):\n",
    "        self.verbose = verbose\n",
    "        self.input_size = input_size\n",
    "        self.hidden1_size = hidden1_size\n",
    "        self.hidden2_size = hidden2_size\n",
    "        self.output_size = output_size\n",
    "        self.sigma = sigma\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.act = act\n",
    "        self.opt = opt\n",
    "        \n",
    "        \n",
    "    def forward(self, X):\n",
    "        # イテレーション毎の順伝搬\n",
    "        # 入力1層目が1dの為、reshapeをかける\n",
    "        self.x = X.reshape(-1)\n",
    "        self.A1 = self.FC1.forward(self.x)\n",
    "        A1 = self.A1.reshape(self.batch_size, -1)\n",
    "        Z1 = self.activation1.forward(A1)\n",
    "        A2 = self.FC2.forward(Z1)\n",
    "        Z2 = self.activation2.forward(A2)\n",
    "        A3 = self.FC3.forward(Z2)\n",
    "        Z3 = self.activation3.forward(A3)\n",
    "        return Z3\n",
    "    \n",
    "    def backward(self, y):\n",
    "        # イテレーション毎の逆伝搬\n",
    "        dA3 = self.activation3.backward(y, self.activation3.Z) # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "        dZ2 = self.FC3.backward(dA3)\n",
    "        dA2 = self.activation2.backward(dZ2)\n",
    "        dZ1 = self.FC2.backward(dA2)\n",
    "        dA1 = self.activation1.backward(dZ1)\n",
    "        # 逆伝搬最終層は1dの為、reshapeをかける\n",
    "        self.dA1 = dA1.reshape(-1)\n",
    "        dZ0 = self.FC1.backward(self.dA1) # dZ0は使用しない\n",
    "    \n",
    "    # 正解率を出力\n",
    "    def accuracy(self, y, z):\n",
    "        return (z.argmax(axis=1) == y).sum()\n",
    "        \n",
    "    def fit(self, X, y, X_val=None, y_val=None, epochs=20):\n",
    "        \n",
    "        # self.sigma : ガウス分布の標準偏差\n",
    "        # self.lr : 学習率\n",
    "        # self.hidden1_size : 1層目のノード数\n",
    "        # self.hidden2_size : 2層目のノード数\n",
    "        # self.output_size : 出力層のノード数\n",
    "        # 各層の設定\n",
    "        # optimizerの設定\n",
    "        if self.opt == None:\n",
    "            optimizer = SGD(self.lr)\n",
    "            \n",
    "        elif self.opt == 'Ada':\n",
    "            optimizer = AdaGrad(self.lr)\n",
    "            \n",
    "        # activateの設定\n",
    "        if self.act == acttanh:\n",
    "            initial = SimpleInitializer(self.sigma)\n",
    "            \n",
    "        elif self.act == actsigmoid:\n",
    "            initial = Xavier(self.sigma)\n",
    "            \n",
    "        elif self.act == actrelu:\n",
    "            initial = He(self.sigma)\n",
    "            \n",
    "        \n",
    "        \n",
    "        self.FC1 = SimpleConv1d(stride=1, padding=0)\n",
    "        self.activation1 = self.act() # tanh\n",
    "        self.FC2 = FC(self.hidden1_size, self.hidden2_size, initial, optimizer)\n",
    "        self.activation2 = self.act() # tanh\n",
    "        self.FC3 = FC(self.hidden2_size, self.output_size, initial, optimizer)\n",
    "        self.activation3 = actsoftmax() # Softmax\n",
    "        div_iter = 600\n",
    "        plot_data = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size)\n",
    "            val_get_mini_batch = GetMiniBatch(X_val, y_val, batch_size=self.batch_size)\n",
    "            for i in range(len(get_mini_batch)):\n",
    "                X_train, y_train = get_mini_batch[i]\n",
    "                X_val2, y_val2 = val_get_mini_batch[i]\n",
    "                batch_size = X_train.shape[0]\n",
    "                self.X = X_train.reshape(batch_size, 784)\n",
    "                self.y = (y_train.reshape(-1, 1) == np.arange(10)).astype(np.float64)\n",
    "\n",
    "#                 print(self.activation3.Z)\n",
    "                Z3 = self.forward(self.X)\n",
    "                loss = self.activation3.loss_func(self.y, Z3)\n",
    "#                 print(f'{i+1} / {len(get_mini_batch)} | train loss:{loss}')\n",
    "                self.backward(self.y)\n",
    "                \n",
    "#                 self.y_val = (y_val2.reshape(-1, 1) == np.arange(10)).astype(np.float64)\n",
    "#                 self.X_val = X_val2.reshape(batch_size, 784)\n",
    "#                 val_Z3 = self.forward(self.X_val)\n",
    "#                 val_loss = self.activation3.loss_func(self.y_val, val_Z3)\n",
    "#                 val_accuracy = self.accuracy(y_val2, self.activation3.Z)\n",
    "                    \n",
    "                print(f'epoch: {epoch + 1} / {epochs}, iteration: {i + 1} / {len(get_mini_batch)} | train loss : {loss:.3}')\n",
    "                    \n",
    "                iters_per_epoch = len(X) / self.batch_size\n",
    "                plot_data.append((epoch + (i + 1) / iters_per_epoch, loss))\n",
    "                    \n",
    "        if self.verbose:\n",
    "#             epochs, train_loss, val_loss = zip(*plot_data)\n",
    "            epochs, train_loss = zip(*plot_data)\n",
    "            plt.plot(epochs, train_loss, color='g', label='train loss')\n",
    "#             plt.plot(epochs, val_loss, color='b', label='val loss')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.ylabel('loss')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "                \n",
    "    def predict(self, X, y):\n",
    "        test_get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size)\n",
    "        result_array = np.array([])\n",
    "        for i in range(len(test_get_mini_batch)):\n",
    "            X_test, _ = test_get_mini_batch[i]\n",
    "            self.X_test = X_test.reshape(self.batch_size, 784)\n",
    "            Z3 = self.forward(self.X)\n",
    "            if len(result_array) == 0:\n",
    "                result_array = self.activation3.Z\n",
    "            else:\n",
    "                result_array = np.append(result_array, self.activation3.Z)\n",
    "                \n",
    "        result_array = result_array.reshape(-1, 10)\n",
    "            \n",
    "        return np.argmax(result_array, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ミニバッチ処理のサンプルクラス\n",
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (10000, 784)\n",
      "0.0 1.0\n",
      "0.0 1.0\n",
      "(48000, 784) (12000, 784)\n",
      "(48000,) (12000,)\n"
     ]
    }
   ],
   "source": [
    "# MNISTダウンロード\n",
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 用意した画像データセットを(サンプル数, 一次元の画素数)型に変換\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "# 画素値を正規化処理\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.min(), X_train.max())\n",
    "print(X_test.min(), X_test.max())\n",
    "\n",
    "# trainとvalデータに分割\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "print(X_train.shape, X_val.shape)\n",
    "print(y_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-1c1b34b9c584>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 3層, 初期値ガウス分布、tanh使用の結果\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mScratchDeepNeuralNetworkClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-1bb7ef79a377>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, X_val, y_val, epochs)\u001b[0m\n\u001b[1;32m    337\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;31m#                 print(f'{i+1} / {len(get_mini_batch)} | train loss:{loss}')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;31m#                 self.y_val = (y_val2.reshape(-1, 1) == np.arange(10)).astype(np.float64)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-1bb7ef79a377>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;31m# 逆伝搬最終層は1dの為、reshapeをかける\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdA1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdA1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         \u001b[0mdZ0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFC1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdA1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# dZ0は使用しない\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;31m# 正解率を出力\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-1bb7ef79a377>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, dA)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0mloss_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mloss_index\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mloss_index\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mdA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 3層, 初期値ガウス分布、tanh使用の結果\n",
    "network = ScratchDeepNeuralNetworkClassifier()\n",
    "network.fit(X_train, y_train, X_val, y_val, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "予測結果の正解率: 0.1135\n"
     ]
    }
   ],
   "source": [
    "pred = network.predict(X_test, y_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "print(f'予測結果の正解率: {metrics.accuracy_score(y_test, pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習はできてはいるが、今回の実装だとエポックを増やしてもlossの改善が見られなかった。<br>\n",
    "また、１次元畳み込み層を1層目としたが、フィルタサイズの設定を力業で行っている。<br>\n",
    "その為、入力データサイズの変化に対応できず、不便な実装になってしまっている。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
